/*:
 # Big O Notation
 
 ## Definition:
 Big O notation is a mathematical notation used in computer science to describe the performance or complexity of an algorithm. Specifically, it characterizes the runtime or space requirements (memory usage) of an algorithm as the size of the input to the algorithm increases. Big O notation provides an upper bound on the growth rate of an algorithm’s runtime or space requirements, offering a way to compare the efficiency of different algorithms when scaling up.
 
 
 
 ##  Key points to understand Big O notation:
 1- **Upper Bound:** Big O gives the worst-case scenario of an algorithm’s growth rate, ensuring that the algorithm will not exceed this bound, but it might perform better.
 
 2- **Input Size:** The notation usually expresses complexity in terms of the size of the input (denoted as "n") to the algorithm, emphasizing how the algorithm scales as the input grows.
 
 3- **Asymptotic Behaviour:** It focuses on the behaviour of an algorithm on the asymptotic limit*
 */
